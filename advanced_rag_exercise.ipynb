{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Exercise\n",
    "\n",
    "This notebook is designed as an exercise to build a complete Retrieval-Augmented Generation (RAG) system. In this exercise, you will integrate three main components into a single pipeline:\n",
    "\n",
    "1. **Retrieval Module** ‚Äì Retrieve relevant documents based on a query.\n",
    "2. **Transformation Module** ‚Äì Transform the retrieved queries.\n",
    "3. **Generation Module and Evaluation** ‚Äì Use the transformed data to generate responses and evaluate the overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Building the RAG Pipeline\n",
    "\n",
    "Load the data and store it in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Asthma: diagnosis, \\nmoni toring and chr onic \\nasth'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üìö Importiere ben√∂tigte Bibliotheken\n",
    "from PyPDF2 import PdfReader          # Zum Lesen von PDF-Dateien\n",
    "import glob                           # Zum Finden von Dateien basierend auf einem Pfad-Muster\n",
    "import tqdm                           # F√ºr eine visuelle Fortschrittsanzeige beim Durchlaufen der Dateien\n",
    "\n",
    "# üìÇ Definiere den Pfad zu allen PDF-Dateien im Verzeichnis \"data/\"\n",
    "# Das Sternchen (*) bedeutet: alle Dateien mit der Endung .pdf\n",
    "glob_path = \"data/*.pdf\"\n",
    "\n",
    "# üìù Lege einen leeren String an, um den extrahierten Text aller PDFs zu sammeln\n",
    "text = \"\"\n",
    "\n",
    "# üîÅ Durchlaufe alle PDF-Dateien im angegebenen Pfad mit Fortschrittsanzeige\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "\n",
    "    # üìÑ √ñffne die aktuelle PDF-Datei im Lese-/Bin√§rmodus\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)  # Erstelle ein PDF-Reader-Objekt\n",
    "\n",
    "        # üßæ Extrahiere den Text von allen Seiten, falls Text vorhanden ist\n",
    "        text += \" \".join(\n",
    "            page.extract_text()           # Text extrahieren\n",
    "            for page in reader.pages      # F√ºr jede Seite im PDF\n",
    "            if page.extract_text()        # Nur wenn Text vorhanden ist (keine leere Seite)\n",
    "        )\n",
    "\n",
    "# üëÅÔ∏è Zeige die ersten 50 Zeichen des zusammengef√ºgten Textes als Vorschau\n",
    "text[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Importiere den Textsplitter (falls noch nicht geschehen)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# üîß Erstelle einen Text-Splitter mit folgenden Parametern:\n",
    "# - chunk_size: max. 2000 Zeichen pro Chunk\n",
    "# - chunk_overlap: jeweils 200 Zeichen √úberlappung zwischen zwei Chunks\n",
    "#   ‚Üí sorgt daf√ºr, dass der Kontext beim √úbergang zwischen Chunks nicht verloren geht\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,         # Maximale L√§nge eines Chunks in Zeichen\n",
    "    chunk_overlap=200        # √úberlappung zwischen benachbarten Chunks\n",
    ")\n",
    "\n",
    "# ‚úÇÔ∏è Teile den extrahierten PDF-Text mithilfe des Splitters in kleinere, √ºberlappende Textabschnitte\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# üßæ Jetzt enth√§lt die Variable 'chunks' eine Liste von Textabschnitten, die jeweils max. 2000 Zeichen lang sind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 130\n",
      "Preview of the first chunk: Asthma: diagnosis, \n",
      "moni toring and chr onic \n",
      "asthma manag emen t (BTS, \n",
      "NICE, SI GN) \n",
      "NICE guideline \n",
      "Published: 27 No vember 202 4 \n",
      "www .nice.or g.uk/guidance/ng2 45 \n",
      "¬© NICE 202 4. All right s reser\n"
     ]
    }
   ],
   "source": [
    "# üìä Zeige die Gesamtanzahl der erzeugten Text-Chunks an\n",
    "# Das ist hilfreich zur Kontrolle, wie viele Abschnitte aus dem PDF-Text entstanden sind\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# üëÅÔ∏è‚Äçüó®Ô∏è Zeige eine Vorschau auf den ersten Chunk (die ersten 200 Zeichen)\n",
    "# So kannst du kontrollieren, ob die Aufteilung sinnvoll funktioniert hat\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an embedding model\n",
    "Use the SentenceTransfomer wrapper as we have done so far.\n",
    "Models are found here: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "or on HuggingFace.\n",
    "\n",
    "Embed the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere den Namen des Embedding-Modells, das verwendet werden soll.\n",
    "# Dieses Modell wurde trainiert, um semantisch √§hnliche S√§tze in √§hnliche Vektoren zu √ºbersetzen.\n",
    "# \"multilingual\" bedeutet, dass es mit mehreren Sprachen (z.‚ÄØB. Englisch, Deutsch) umgehen kann.\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# Lade das ausgew√§hlte Modell mit der SentenceTransformer-Bibliothek.\n",
    "# Es wird intern von HuggingFace geladen und kann sofort zur Vektorisierung verwendet werden.\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Erzeuge Embeddings (Vektoren) f√ºr alle Text-Chunks.\n",
    "# convert_to_numpy=True sorgt daf√ºr, dass du ein NumPy-Array zur√ºckbekommst (praktisch f√ºr sp√§tere Verarbeitung).\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Index and save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# üìê Ermittle die Anzahl der Dimensionen eines einzelnen Embedding-Vektors\n",
    "# chunk_embeddings ist ein 2D-Array mit der Form (Anzahl der Chunks, Anzahl der Dimensionen)\n",
    "# z.‚ÄØB. (120, 384) ‚Üí 120 Chunks, jeder als Vektor mit 384 Werten\n",
    "\n",
    "d = chunk_embeddings.shape[1]  # Index [1] gibt die Spaltenanzahl = Vektor-Dimension\n",
    "\n",
    "# üñ®Ô∏è Gib die Dimension des Embeddings aus (wichtig f√ºr FAISS oder √Ñhnlichkeitsvergleiche)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¢ Was bedeutet das Ergebnis 384?\n",
    "Das Ergebnis 384 bedeutet, dass jedes deiner Chunks durch das Embedding-Modell in einen Vektor mit 384 Dimensionen umgewandelt wurde.\n",
    "\n",
    "üîç Was hei√üt das konkret?\n",
    "Du hattest eine Liste mit vielen Chunks, z.‚ÄØB.:\n",
    "\n",
    "python\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "chunks = [\"Textabschnitt 1\", \"Textabschnitt 2\", ...]\n",
    "Mit dem Embedding-Modell:\n",
    "\n",
    "python\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "wurde jeder Chunk in einen Vektor umgerechnet.\n",
    "\n",
    "Jeder dieser Vektoren enth√§lt 384 Zahlenwerte, z.‚ÄØB.:\n",
    "\n",
    "csharp\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "[0.12, -0.34, 0.87, ..., 0.01]  ‚Üí  L√§nge: 384\n",
    "üí° Warum genau 384?\n",
    "Das liegt am verwendeten Modell:\n",
    "\n",
    "paraphrase-multilingual-MiniLM-L12-v2 erzeugt standardm√§√üig 384-dimensionale Embeddings.\n",
    "\n",
    "üîÅ Andere Modelle liefern z.‚ÄØB.:\n",
    "\n",
    "768 (z.‚ÄØB. BERT-Base)\n",
    "\n",
    "1024 (z.‚ÄØB. RoBERTa-Large)\n",
    "\n",
    "üìå Bedeutung in der Praxis\n",
    "Du kannst dir jeden Vektor als Punkt in einem 384-dimensionalen Raum vorstellen.\n",
    "\n",
    "Zwei Chunks, die semantisch √§hnlich sind, liegen nahe beieinander in diesem Raum.\n",
    "\n",
    "Das bildet die Grundlage f√ºr:\n",
    "\n",
    "‚úÖ semantische Suche\n",
    "\n",
    "‚úÖ Clustering\n",
    "\n",
    "‚úÖ Klassifikation\n",
    "\n",
    "‚úÖ √Ñhnlichkeitsvergleiche\n",
    "\n",
    "üß† Wichtig f√ºr FAISS\n",
    "Wenn du z.‚ÄØB. mit FAISS einen Vektorindex aufbaust, musst du die Vektordimension angeben. In deinem Fall ist das:\n",
    "\n",
    "python\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "d = 384\n",
    "FAISS erwartet diese Angabe, um korrekt mit den Vektoren arbeiten zu k√∂nnen.\n",
    "\n",
    "M√∂chtest du auch ein kurzes Beispiel mit np.linalg.norm() oder Cosinus-√Ñhnlichkeit zur Visualisierung von Vektorabstand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 130\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Erstelle einen FAISS-Index zur schnellen √Ñhnlichkeitssuche\n",
    "# Wir verwenden hier \"IndexFlatL2\", der auf der euklidischen Distanz (L2) basiert.\n",
    "# Der Parameter `d` gibt die Anzahl der Dimensionen pro Vektor an (z.‚ÄØB. 384 bei deinem Modell).\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# ‚ûï F√ºge alle zuvor erzeugten Embedding-Vektoren in den Index ein\n",
    "# Dadurch kann FAISS sp√§ter Anfragen (Queries) mit diesen vergleichen\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "# üî¢ Gib aus, wie viele Vektoren im Index gespeichert sind\n",
    "# Sollte gleich der Anzahl deiner Chunks sein\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Speichere den FAISS-Index auf der Festplatte\n",
    "# ‚Üí Damit musst du den Index beim n√§chsten Mal nicht neu berechnen\n",
    "#    (spart Zeit beim sp√§teren Wiederverwenden)\n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "\n",
    "# üóÇÔ∏è Speichere zus√§tzlich die Text-Chunks als Mapping (Index ‚Üí Originaltext)\n",
    "# ‚Üí So kannst du sp√§ter zu jedem Treffer die zugeh√∂rige Textpassage finden\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)  # Serialisiere und speichere die Liste der Chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Key for language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Lade Umgebungsvariablen aus einer .env-Datei (falls vorhanden)\n",
    "# Die Funktion `load_dotenv()` sucht nach einer Datei namens `.env` im Projektverzeichnis\n",
    "# und l√§dt alle darin enthaltenen Variablen als Umgebungsvariablen (environment variables).\n",
    "load_dotenv()\n",
    "\n",
    "# üîê Lies den Google-API-Schl√ºssel aus der Umgebungsvariable GOOGLE_API_KEY\n",
    "# Wenn die Variable nicht gesetzt ist, wird `None` zur√ºckgegeben ‚Äì es erfolgt KEIN Fehler.\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# üîê Lies den OpenAI-API-Schl√ºssel aus der Umgebungsvariable OPENAI_API_KEY\n",
    "# Auch hier: kein Fehler, falls der Schl√ºssel nicht vorhanden ist.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# üîê Lies den Groq-API-Schl√ºssel aus der Umgebungsvariable GROQ_API_KEY\n",
    "# Der R√ºckgabewert ist `None`, falls keine solche Umgebungsvariable existiert.\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# üí° Verhalten:\n",
    "# - Wenn keine `.env`-Datei vorhanden ist, oder eine Variable fehlt,\n",
    "#   dann sind die entsprechenden Variablen (`google_api_key`, etc.) = `None`.\n",
    "# - Das l√∂st an dieser Stelle **noch keinen Fehler aus**.\n",
    "# - Fehler treten erst auf, wenn du z.‚ÄØB. versuchst, einen LLM-Client mit einem `None`-Key zu initialisieren.\n",
    "#\n",
    "# ‚úÖ Optionaler Schutz (empfohlen):\n",
    "# assert google_api_key is not None, \"GOOGLE_API_KEY not found!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a retriever function\n",
    "\n",
    "arguments: query, k, index, chunks, embedding model\n",
    "\n",
    "return: retrieved texts, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_texts(query, k, index, token_split_texts, model):\n",
    "    \"\"\"\n",
    "    üîé Funktion zur semantischen Suche:\n",
    "    Gibt die top-k √§hnlichsten Text-Chunks (und ihre Distanzen) zu einer gegebenen Anfrage (query) zur√ºck.\n",
    "\n",
    "    Parameter:\n",
    "    - query: Die vom Benutzer gestellte Suchfrage (Textstring)\n",
    "    - k: Anzahl der gew√ºnschten √§hnlichsten Treffer (z.‚ÄØB. 3)\n",
    "    - index: FAISS-Index mit gespeicherten Embeddings der Chunks\n",
    "    - token_split_texts: Die Original-Textchunks (z.‚ÄØB. nach Tokenisierung)\n",
    "    - model: Das Embedding-Modell zur Umwandlung der Anfrage in einen Vektor\n",
    "\n",
    "    R√ºckgabe:\n",
    "    - retrieved_texts: Liste der k √§hnlichsten Textabschnitte (aus den Chunks)\n",
    "    - distances: Liste der zugeh√∂rigen Distanzen (kleiner = √§hnlicher)\n",
    "    \"\"\"\n",
    "    \n",
    "    # üìê Wandelt die Textanfrage in einen Embedding-Vektor um\n",
    "    # `convert_to_numpy=True` liefert ein NumPy-Array, wie es FAISS erwartet\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # üîç Durchsuche den FAISS-Index mit dem erzeugten Vektor\n",
    "    # Gibt die k √§hnlichsten Vektoren zur√ºck (nach euklidischer Distanz)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # üìã Extrahiere die zugeh√∂rigen Original-Text-Chunks basierend auf den gefundenen Indizes\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    \n",
    "    # üîÅ Gib die Treffer und ihre Distanzen zur√ºck\n",
    "    return retrieved_texts, distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build an answer function\n",
    "Build an answer function that takes a query, k, an index and the chunks.\n",
    "\n",
    "return: answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, k, index, texts):\n",
    "    \"\"\"\n",
    "    üì• Diese Funktion beantwortet eine Benutzerfrage basierend auf den k √§hnlichsten Chunks.\n",
    "    Sie verwendet semantische Suche (FAISS), um relevante Kontexte zu finden,\n",
    "    und ein LLM (√ºber Groq), um daraus eine Antwort zu generieren.\n",
    "    \n",
    "    Parameter:\n",
    "    - query (str): Die Benutzerfrage.\n",
    "    - k (int): Anzahl der zu verwendenden √§hnlichsten Chunks.\n",
    "    - index: FAISS-Index mit Embeddings.\n",
    "    - texts: Die urspr√ºnglichen Chunks (z.‚ÄØB. aus PDF-Texten).\n",
    "\n",
    "    R√ºckgabe:\n",
    "    - answer (str): Die generierte Antwort des LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    # üß† Initialisiere das Embedding-Modell (gleich wie beim Erstellen des FAISS-Index)\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # üîç Verwende die Retriever-Funktion, um die k √§hnlichsten Chunks zur query zu finden\n",
    "    retrieved_texts, _ = retrieve_texts(query, k, index, texts, model)\n",
    "\n",
    "    # üìö Fasse die gefundenen Text-Chunks zu einem Kontextblock zusammen\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    # üßæ Baue den Prompt: Erkl√§re die Antwort kindgerecht auf Basis des Kontexts\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        \"Question: \" + query + \"\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # üîê Initialisiere den Groq-Client mit dem API-Key\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "\n",
    "    # üí¨ Baue die Nachricht im OpenAI-kompatiblen Format\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",  # Rolle: System gibt Instruktionen vor\n",
    "            \"content\": prompt  # Prompt mit Kontext + Frage\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # ü§ñ Anfrage an das LLM (hier: llama3-70b √ºber Groq)\n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "\n",
    "    # üì§ Extrahiere und gib die Antwort zur√ºck\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the most important factor in diagnosing asthma?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ü§ñ Rufe die Antwortfunktion auf, um die Query durch semantische Suche + LLM beantworten zu lassen\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# - Nutzt die top-5 √§hnlichsten Chunks (k = 5)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# - Verwendet den FAISS-Index (index) und die Original-Chunks (chunks)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43manswer_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# üì§ Gib die generierte Antwort aus\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[0;32mIn[41], line 48\u001b[0m, in \u001b[0;36manswer_query\u001b[0;34m(query, k, index, texts)\u001b[0m\n\u001b[1;32m     40\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     41\u001b[0m     {\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Rolle: System gibt Instruktionen vor\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt  \u001b[38;5;66;03m# Prompt mit Kontext + Frage\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     }\n\u001b[1;32m     45\u001b[0m ]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# ü§ñ Anfrage an das LLM (hier: llama3-70b √ºber Groq)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.3-70b-versatile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# üì§ Extrahiere und gib die Antwort zur√ºck\u001b[39;00m\n\u001b[1;32m     54\u001b[0m answer \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/resources/chat/completions.py:322\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1224\u001b[0m     )\n\u001b[0;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:917\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/groq/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1029\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# ‚ùì Definiere die Benutzerfrage (Query), die beantwortet werden soll\n",
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "\n",
    "# ü§ñ Rufe die Antwortfunktion auf, um die Query durch semantische Suche + LLM beantworten zu lassen\n",
    "# - Nutzt die top-5 √§hnlichsten Chunks (k = 5)\n",
    "# - Verwendet den FAISS-Index (index) und die Original-Chunks (chunks)\n",
    "answer = answer_query(query, 5, index, chunks)\n",
    "\n",
    "# üì§ Gib die generierte Antwort aus\n",
    "print(\"LLM Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Rewriter\n",
    "\n",
    "Take a query and an api key for the model and rewrite the query. \n",
    "\n",
    "Rewriting a query: A Language Model is prompted to rewrite a query to better suit a task.\n",
    "\n",
    "Other Transfomrations are implemented in a similar fashion, this is just an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query, groq_api_key):\n",
    "    \"\"\"\n",
    "    ‚úçÔ∏è Rewrite the user's query to better suit medical guideline retrieval.\n",
    "\n",
    "    Ziel:\n",
    "    - Formuliere die urspr√ºngliche Anfrage so um, dass sie pr√§ziser und besser f√ºr die Suche in medizinischen Richtlinien geeignet ist.\n",
    "\n",
    "    Parameter:\n",
    "    - query (str): Die originale Benutzeranfrage (z.‚ÄØB. \"Tell me more about asthma\").\n",
    "    - groq_api_key (str): Dein Groq API Key zur Authentifizierung des LLM-Zugriffs.\n",
    "\n",
    "    R√ºckgabe:\n",
    "    - rewritten_query (str): Die umformulierte Anfrage, angepasst an den medizinischen Kontext.\n",
    "    \"\"\"\n",
    "\n",
    "    # üß† Initialisiere den Groq-Client mit deinem API-Schl√ºssel\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "\n",
    "    # üßæ Prompt f√ºr das Sprachmodell, um die Anfrage umzuschreiben\n",
    "    rewriting_prompt = (\n",
    "        \"Rewrite the following query into a format, such that it can be answered by looking at medical guidelines. \"\n",
    "        \"Keep the keywords but ensure that it is close to a format, such as in medical guidelines. \"\n",
    "        \"Just answer with the rewritten query\\n\\n\"\n",
    "        \"Query: \" + query\n",
    "    )\n",
    "\n",
    "    # üì® Baue die Nachrichtenstruktur f√ºr das Chat-Modell (nur eine Systemnachricht mit dem Prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rewriting_prompt}\n",
    "    ]\n",
    "\n",
    "    # ü§ñ Sende die Anfrage an das LLM (hier: llama-3.3-70b-versatile) und erhalte die Antwort\n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    # ‚úÇÔ∏è Extrahiere die Antwort aus der LLM-Antwortstruktur und entferne Whitespace\n",
    "    rewritten_query = llm.choices[0].message.content.strip()\n",
    "\n",
    "    # üîô Gib die umgeschriebene Anfrage zur√ºck\n",
    "    return rewritten_query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the rewriter into your answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_rewriting(query, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    ‚ùì Funktion zur Beantwortung einer Nutzeranfrage mit Rewriting und semantischer Suche.\n",
    "    Diese Version verwendet zus√§tzlich eine Rewriter-Funktion, um die urspr√ºngliche Frage\n",
    "    zu optimieren, bevor sie verarbeitet wird.\n",
    "\n",
    "    Parameter:\n",
    "    - query (str): Die urspr√ºngliche Frage des Benutzers.\n",
    "    - k (int): Anzahl der zur√ºckzugebenden √§hnlichsten Text-Chunks.\n",
    "    - index: FAISS-Index, der alle gespeicherten Embeddings enth√§lt.\n",
    "    - texts (list): Liste der urspr√ºnglichen Chunks (Textabschnitte).\n",
    "    - groq_api_key (str): API-Schl√ºssel f√ºr das Groq-LLM.\n",
    "\n",
    "    R√ºckgabe:\n",
    "    - answer (str): Die vom Language Model generierte Antwort.\n",
    "    \"\"\"\n",
    "\n",
    "    # üß† Lade das Embedding-Modell, um Abfragen in Vektoren umzuwandeln\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # ‚úèÔ∏è Wende den Rewriter auf die urspr√ºngliche Frage an,\n",
    "    # um sie besser f√ºr die semantische Suche geeignet zu machen\n",
    "    rewritten_query = rewrite_query(query, groq_api_key)    \n",
    "    print(\"Rewritten Query:\", rewritten_query)  # Debug-Ausgabe f√ºr die √ºberarbeitete Frage\n",
    "\n",
    "    # üîç Rufe die semantisch √§hnlichsten Text-Chunks ab (auf Basis der rewritten_query)\n",
    "    retrieved_texts, _ = retrieve_texts(rewritten_query, k, index, texts, model)\n",
    "\n",
    "    # üìö Kombiniere die abgerufenen Chunks zu einem Kontext-Block\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "\n",
    "    # üìù Erstelle einen Prompt, der den Kontext und die urspr√ºngliche Frage enth√§lt\n",
    "    # und das Modell auffordert, die Antwort kindgerecht zu formulieren\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        \"Question: \" + query + \"\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # ü§ñ Initialisiere den Groq-Client mit deinem API-Key\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "\n",
    "    # üí¨ Formuliere die Nachricht f√ºr das LLM im System-Kontext\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # üß† Verwende das LLM (hier: llama-3.3-70b-versatile) zur Generierung der Antwort\n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "\n",
    "    # üì§ Extrahiere und gib die generierte Antwort zur√ºck\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten Query: What are the diagnostic criteria for asthma, and which clinical feature or symptom is considered the primary indicator for a definitive diagnosis according to current clinical guidelines?\n",
      "LLM Answer: Oh boy, let's talk about asthma. So, you know how sometimes people have trouble breathing and it can be really scary? Well, doctors need to figure out if someone has asthma or not. \n",
      "\n",
      "The most important thing in diagnosing asthma is looking at how someone's body reacts when they breathe. Doctors can do special tests like spirometry, FeNO measurements, or skin prick tests to see how their lungs work. They can also check for something called IgE in their blood, which can help them understand if someone has allergies that might be making their asthma worse.\n",
      "\n",
      "But, the best way to know for sure if someone has asthma is to do a special test called a bronchial challenge test. This test is like a big puzzle that helps doctors understand how someone's lungs react to different things. \n",
      "\n",
      "Sometimes, doctors might not have all the tools they need to do these tests, so they might use other ways to guess if someone has asthma. Like, they might look at how often someone gets sick or if they have a lot of allergy symptoms. \n",
      "\n",
      "It's kind of like solving a mystery, and doctors need to use all the clues they can find to figure out if someone has asthma or not!\n"
     ]
    }
   ],
   "source": [
    "# üßæ Definiere die urspr√ºngliche Nutzerfrage\n",
    "# Diese Frage wird sp√§ter durch die Rewriter-Funktion √ºberarbeitet\n",
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "\n",
    "# üß† Rufe die Hauptfunktion zur Beantwortung der Frage auf\n",
    "# Diese Funktion:\n",
    "# - rewritten die Frage mit Hilfe eines LLMs\n",
    "# - sucht die top 5 (k=5) √§hnlichsten Chunks aus dem FAISS-Index\n",
    "# - erstellt daraus einen Kontext\n",
    "# - und ruft dann ein Language Model (LLM) auf, um basierend auf diesem Kontext eine Antwort zu generieren\n",
    "answer = answer_query_with_rewriting(query, 5, index, chunks, groq_api_key)\n",
    "\n",
    "# üñ®Ô∏è Gib die Antwort des LLMs im Notebook aus\n",
    "print(\"LLM Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 .Evaluation\n",
    "\n",
    "Select random chunks from all your chunks, and generate a question to each of these chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import httpx  # Ensure you're catching the correct timeout exception\n",
    "from openai import OpenAI\n",
    "def generate_questions_for_random_chunks(chunks, num_chunks=20, max_retries=3):\n",
    "    \"\"\"\n",
    "    Randomly selects a specified number of text chunks from the provided list,\n",
    "    then generates a question for each selected chunk using the Groq LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - num_chunks (int): Number of chunks to select randomly (default is 20).\n",
    "\n",
    "    Returns:\n",
    "    - questions (list of tuples): Each tuple contains (chunk, generated_question).\n",
    "    \"\"\"\n",
    "    # Randomly select the desired number of chunks.\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    \n",
    "    # Initialize the Groq client once\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    questions = []\n",
    "    for chunk in tqdm.tqdm(selected_chunks):\n",
    "        # Build a prompt that asks the LLM to generate a question based on the chunk.\n",
    "        prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        generated_question = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Try calling the API with simple retry logic.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                     model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "                generated_question = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred for chunk. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)  # Wait a bit before retrying.\n",
    "        \n",
    "        # If all attempts fail, use an error message as the generated question.\n",
    "        if generated_question is None:\n",
    "            generated_question = \"Error: Failed to generate question after several retries.\"\n",
    "        \n",
    "        questions.append((chunk, generated_question))\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:05<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "52 ‚Ä¢ Follow the recommendations f or disease-specific indications in t he NICE \n",
      "guideline on t heir ...\n",
      "Generated Question: What specific antihypertensive drug treatments does the NICE guideline recommend for adults with type 2 diabetes or those under 55, and how do these recommendations vary based on genetic background?\n",
      "\n",
      "Chunk 2:\n",
      "conditions such as f ood aller gies. \n",
      "Bronchial challeng e test \n",
      "A test t o measur e airway responsi...\n",
      "Generated Question: What are the various tests and measures used to assess airway responsiveness and inflammation in asthma, and what do they indicate about the condition?\n",
      "\n",
      "Chunk 3:\n",
      "developed t o suppor t healt hcare professionals and people wit h hyper tension t o discuss \n",
      "their t...\n",
      "Generated Question: What factors contribute to the committee's decision to retain and update the existing treatment recommendations for step 4 hypertension management, despite the absence of new evidence?\n",
      "\n",
      "Chunk 4:\n",
      "more of t he following: \n",
      "‚Ä¢ target or gan damage \n",
      "‚Ä¢ established car diovascular disease \n",
      "‚Ä¢ renal dise...\n",
      "Generated Question: What role does clinical judgment play in the treatment of hypertension for individuals with frailty or multimorbidity according to the NICE guidelines?\n",
      "\n",
      "Chunk 5:\n",
      "BTS ISBN: 9 78-1-917 619-01-1 \n",
      "NICE ISBN: 9 78-1-47 31-6612- 7 \n",
      "SIGN ISBN: 9 78-1-909103-92-4 Asthma...\n",
      "Generated Question: What are the main responsibilities of healthcare professionals and local healthcare providers in implementing NICE guidelines for asthma and hypertension management, and how should they balance these recommendations with individual patient needs?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = generate_questions_for_random_chunks(chunks, num_chunks=5, max_retries=2)\n",
    "for idx, (chunk, question) in enumerate(questions, start=1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk[:100]}...\\nGenerated Question: {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Test the questions with your built retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    For each (chunk, generated_question) tuple in the provided list, use the prebuilt\n",
    "    retrieval function to generate an answer for the generated question. The function\n",
    "    returns a list of dictionaries containing the original chunk, the generated question,\n",
    "    and the answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - question_tuples (list of tuples): Each tuple is (chunk, generated_question)\n",
    "    - k (int): Number of retrieved documents to use for answering.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list of dict): Each dict contains 'chunk', 'question', and 'answer'.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        # Use your retrieval-based answer function. Here we assume the function signature is:\n",
    "        # answer_query(query, k, index, texts, groq_api_key)\n",
    "        answer = answer_query(question, k, index, texts) #query, k, index,texts\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Preview: 52 ‚Ä¢ Follow the recommendations f or disease-specific indications in t he NICE \n",
      "guideline on t heir \n",
      "Generated Question: What specific antihypertensive drug treatments does the NICE guideline recommend for adults with type 2 diabetes or those under 55, and how do these recommendations vary based on genetic background?\n",
      "Answer: So, you know how sometimes we talk about medicine and doctors? Well, there's a special group called NICE that helps doctors know what medicines to give to people who are sick. \n",
      "\n",
      "For people with high blood pressure, NICE says that if you have type 2 diabetes, or if you're under 55 years old, they recommend a special kind of medicine called an ACE inhibitor or an ARB. It's like a special kind of helper that makes your blood pressure go down.\n",
      "\n",
      "But, if you're from a family that came from Black Africa or the Caribbean, the doctors might choose a different medicine for you. That's because some medicines work better for some people than others, depending on where their family came from.\n",
      "\n",
      "It's like when you're playing with blocks, and you need to find the right block to fit in the right spot. Doctors need to find the right medicine to fit the right person, and that's what these recommendations are trying to help them do. \n",
      "\n",
      "So, to sum it up: if you have type 2 diabetes or you're under 55, you might get an ACE inhibitor or an ARB to help with your high blood pressure. And if you're from a Black African or Caribbean family, the doctor might choose a different medicine just for you!\n",
      "-----------------------------\n",
      "Chunk Preview: conditions such as f ood aller gies. \n",
      "Bronchial challeng e test \n",
      "A test t o measur e airway responsi\n",
      "Generated Question: What are the various tests and measures used to assess airway responsiveness and inflammation in asthma, and what do they indicate about the condition?\n",
      "Answer: Oh boy, let's talk about asthma tests. So, you know how sometimes you might feel like you can't breathe very well? That can be because of something called asthma. And to figure out what's going on, doctors use special tests.\n",
      "\n",
      "Imagine you have a big straw that you breathe through, and sometimes that straw gets a little smaller. That's kind of like what happens with asthma. The tests help doctors see how small the straw is and what might be making it smaller.\n",
      "\n",
      "One test is like a special game where you breathe out really fast, and it measures how much air you can blow out. It's called a peak flow test. Another test is like a special machine that measures the bad guys in your breath, like nitric oxide. That one is called FeNO.\n",
      "\n",
      "These tests help doctors see if your asthma is under control or if you need to make some changes to feel better. They're like special tools that help us take care of our breathing and make sure we can run around and play without getting too tired or feeling icky.\n",
      "\n",
      "But that's not all! Doctors also use something called a symptom questionnaire. It's like a special list of questions that asks how you're feeling, like if you've been coughing a lot or feeling tired. And they also look at how often you need to use your inhaler, which is like a special medicine that helps open up the straw.\n",
      "\n",
      "All these tests and measures help doctors understand what's going on with your asthma and how to make it better. They're like special helpers that make sure we can breathe easily and feel happy and healthy!\n",
      "-----------------------------\n",
      "Chunk Preview: developed t o suppor t healt hcare professionals and people wit h hyper tension t o discuss \n",
      "their t\n",
      "Generated Question: What factors contribute to the committee's decision to retain and update the existing treatment recommendations for step 4 hypertension management, despite the absence of new evidence?\n",
      "Answer: Oh boy, let's talk about something important for grown-ups. So, there's a group of people called a committee, and they help decide how to take care of people with high blood pressure. They want to make sure everyone gets the best treatment possible.\n",
      "\n",
      "When they were looking at how to treat people with very high blood pressure, they didn't find any new information that would change what they already thought was best. But, they still wanted to make sure they were doing everything right.\n",
      "\n",
      "They looked at what they already knew and decided to keep some of the old rules because they worked well and people were already using them. They also wanted to make sure that people were taking their medicine correctly and that doctors were checking their blood pressure often enough.\n",
      "\n",
      "Additionally, they thought about people who might have other health problems, like heart issues or diabetes, and how that might affect their treatment. They wanted to make sure those people got the right care too.\n",
      "\n",
      "So, even though they didn't find new information, they still updated some of their recommendations to make sure everyone gets the best care possible. And that's a good thing! \n",
      "\n",
      "Some of the things they thought about when making their decisions were:\n",
      "\n",
      "* If people were already doing something that worked, why change it?\n",
      "* Were people taking their medicine like they were supposed to?\n",
      "* Were doctors checking blood pressure regularly?\n",
      "* How would other health problems affect treatment?\n",
      "\n",
      "All these things helped the committee make their decisions and keep people safe and healthy.\n",
      "-----------------------------\n",
      "Chunk Preview: more of t he following: \n",
      "‚Ä¢ target or gan damage \n",
      "‚Ä¢ established car diovascular disease \n",
      "‚Ä¢ renal dise\n",
      "Generated Question: What role does clinical judgment play in the treatment of hypertension for individuals with frailty or multimorbidity according to the NICE guidelines?\n",
      "Answer: So, you know how sometimes we get sick or feel icky? And we have to go see a doctor to get better? Well, the NICE guidelines are like a special set of instructions for doctors to help them make people feel better.\n",
      "\n",
      "When it comes to helping people with something called hypertension, or high blood pressure, the doctors have to be very careful. Especially if the person is very old or has lots of other health problems. That's called frailty or multimorbidity.\n",
      "\n",
      "The NICE guidelines say that doctors should use their special judgment to decide how to treat people with frailty or multimorbidity. It's like they have to use their own superpower to figure out what's best for each person.\n",
      "\n",
      "So, the doctor will look at the person's whole body and all their health problems, and then they'll decide how to treat their high blood pressure. It's not just about following a recipe, it's about being very thoughtful and careful. And that's why clinical judgment is so important!\n",
      "-----------------------------\n",
      "Chunk Preview: BTS ISBN: 9 78-1-917 619-01-1 \n",
      "NICE ISBN: 9 78-1-47 31-6612- 7 \n",
      "SIGN ISBN: 9 78-1-909103-92-4 Asthma\n",
      "Generated Question: What are the main responsibilities of healthcare professionals and local healthcare providers in implementing NICE guidelines for asthma and hypertension management, and how should they balance these recommendations with individual patient needs?\n",
      "Answer: Oh boy, let's talk about taking care of people with asthma. So, there are special helpers called doctors and nurses who need to follow some rules to make sure people with asthma get the best care. These rules are like a guidebook that says what's best for people with asthma.\n",
      "\n",
      "The main jobs of these helpers are to:\n",
      "\n",
      "1. **Listen to the patient**: They need to hear what the patient wants and needs, and make decisions together.\n",
      "2. **Give the right medicine**: They need to make sure the patient is taking the right medicine, and that they're taking it correctly.\n",
      "3. **Help the patient make good choices**: They need to teach the patient how to take care of themselves, like how to use their inhaler and how to stay away from things that make their asthma worse.\n",
      "4. **Make sure the patient is safe**: They need to watch out for any problems that might happen when the patient is taking their medicine, and report any issues to the right people.\n",
      "\n",
      "The local healthcare providers, like hospitals and clinics, also have some big jobs:\n",
      "\n",
      "1. **Make sure the helpers have what they need**: They need to give the doctors and nurses the tools and resources they need to take care of patients with asthma.\n",
      "2. **Follow the guidebook**: They need to follow the rules and guidelines that say what's best for people with asthma.\n",
      "3. **Be kind to the Earth**: They need to try to reduce waste and be kind to the environment when they're taking care of patients with asthma.\n",
      "4. **Make sure everyone is treated fairly**: They need to make sure that all patients are treated equally and with respect, no matter who they are or where they come from.\n",
      "\n",
      "When it comes to balancing the guidebook with what's best for each patient, the helpers need to use their brains and their hearts. They need to think about what's best for each patient, and what will make them feel better. They also need to listen to the patient and their family, and work together to make a plan that's just right for them. It's like putting together a puzzle, and making sure all the pieces fit just right!\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "results = answer_generated_questions(questions, 5, index, chunks, groq_api_key)\n",
    "\n",
    "for item in results:\n",
    "    print(\"Chunk Preview:\", item['chunk'][:100])\n",
    "    print(\"Generated Question:\", item['question'])\n",
    "    print(\"Answer:\", item['answer'])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_answers_binary(results, groq_api_key, max_retries=3):\n",
    "    \"\"\"\n",
    "    Evaluates each answer in the results list using an LLM.\n",
    "    For each result (a dictionary containing 'chunk', 'question', and 'answer'),\n",
    "    it sends an evaluation prompt to the Groq LLM which outputs 1 if the answer is on point,\n",
    "    and 0 if it is missing the point.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list of dict): Each dict must contain keys 'chunk', 'question', and 'answer'.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - max_retries (int): Maximum number of retries if the API call times out.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A dataframe containing the original chunk, question, answer, and evaluation score.\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    for item in tqdm.tqdm(results, desc=\"Evaluating Answers\"):\n",
    "        # Build the evaluation prompt.\n",
    "        prompt = (\n",
    "            \"Evaluate the following answer to the given question. \"\n",
    "            \"If the answer is accurate and complete, reply with 1. \"\n",
    "            \"If the answer is inaccurate, incomplete, or otherwise not acceptable, reply with 0. \"\n",
    "            \"Do not include any extra text.\\n\\n\"\n",
    "            \"Question: \" + item['question'] + \"\\n\\n\"\n",
    "            \"Answer: \" + item['answer'] + \"\\n\\n\"\n",
    "            \"Context (original chunk): \" + item['chunk'] + \"\\n\\n\"\n",
    "            \"Evaluation (1 for good, 0 for bad):\"\n",
    "        )\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        generated_eval = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Retry logic in case of timeouts or errors.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=\"4o-mini\"\n",
    "                )\n",
    "                generated_eval = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the retry loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred during evaluation. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error during evaluation: {e}. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # If no valid evaluation was produced, default to 0.\n",
    "        if generated_eval is None:\n",
    "            generated_eval = \"0\"\n",
    "        \n",
    "        # Convert the response to an integer (1 or 0).\n",
    "        try:\n",
    "            score = int(generated_eval)\n",
    "            if score not in [0, 1]:\n",
    "                score = 0\n",
    "        except:\n",
    "            score = 0\n",
    "        \n",
    "        evaluations.append(score)\n",
    "    \n",
    "    # Add the evaluation score to each result.\n",
    "    for i, item in enumerate(results):\n",
    "        item['evaluation'] = evaluations[i]\n",
    "    \n",
    "    # Create a dataframe for manual review.\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  20%|‚ñà‚ñà        | 1/5 [00:06<00:26,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:13<00:19,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:19<00:13,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:26<00:06,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:32<00:00,  6.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52 ‚Ä¢ Follow the recommendations f or disease-s...</td>\n",
       "      <td>What specific antihypertensive drug treatments...</td>\n",
       "      <td>So, you know how sometimes we talk about medic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conditions such as f ood aller gies. \\nBronchi...</td>\n",
       "      <td>What are the various tests and measures used t...</td>\n",
       "      <td>Oh boy, let's talk about asthma tests. So, you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>developed t o suppor t healt hcare professiona...</td>\n",
       "      <td>What factors contribute to the committee's dec...</td>\n",
       "      <td>Oh boy, let's talk about something important f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more of t he following: \\n‚Ä¢ target or gan dama...</td>\n",
       "      <td>What role does clinical judgment play in the t...</td>\n",
       "      <td>So, you know how sometimes we get sick or feel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BTS ISBN: 9 78-1-917 619-01-1 \\nNICE ISBN: 9 7...</td>\n",
       "      <td>What are the main responsibilities of healthca...</td>\n",
       "      <td>Oh boy, let's talk about taking care of people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  52 ‚Ä¢ Follow the recommendations f or disease-s...   \n",
       "1  conditions such as f ood aller gies. \\nBronchi...   \n",
       "2  developed t o suppor t healt hcare professiona...   \n",
       "3  more of t he following: \\n‚Ä¢ target or gan dama...   \n",
       "4  BTS ISBN: 9 78-1-917 619-01-1 \\nNICE ISBN: 9 7...   \n",
       "\n",
       "                                            question  \\\n",
       "0  What specific antihypertensive drug treatments...   \n",
       "1  What are the various tests and measures used t...   \n",
       "2  What factors contribute to the committee's dec...   \n",
       "3  What role does clinical judgment play in the t...   \n",
       "4  What are the main responsibilities of healthca...   \n",
       "\n",
       "                                              answer  evaluation  \n",
       "0  So, you know how sometimes we talk about medic...           0  \n",
       "1  Oh boy, let's talk about asthma tests. So, you...           0  \n",
       "2  Oh boy, let's talk about something important f...           0  \n",
       "3  So, you know how sometimes we get sick or feel...           0  \n",
       "4  Oh boy, let's talk about taking care of people...           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_evaluations = evaluate_answers_binary(results, openai_api_key)\n",
    "display(df_evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
