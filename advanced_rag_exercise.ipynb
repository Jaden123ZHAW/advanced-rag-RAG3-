{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Exercise\n",
    "\n",
    "This notebook is designed as an exercise to build a complete Retrieval-Augmented Generation (RAG) system. In this exercise, you will integrate three main components into a single pipeline:\n",
    "\n",
    "1. **Retrieval Module** – Retrieve relevant documents based on a query.\n",
    "2. **Transformation Module** – Transform the retrieved queries.\n",
    "3. **Generation Module and Evaluation** – Use the transformed data to generate responses and evaluate the overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Building the RAG Pipeline\n",
    "\n",
    "Load the data and store it in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Asthma: diagnosis, \\nmoni toring and chr onic \\nasth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📚 Importiere benötigte Bibliotheken\n",
    "from PyPDF2 import PdfReader          # Zum Lesen von PDF-Dateien\n",
    "import glob                           # Zum Finden von Dateien basierend auf einem Pfad-Muster\n",
    "import tqdm                           # Für eine visuelle Fortschrittsanzeige beim Durchlaufen der Dateien\n",
    "\n",
    "# 📂 Definiere den Pfad zu allen PDF-Dateien im Verzeichnis \"data/\"\n",
    "# Das Sternchen (*) bedeutet: alle Dateien mit der Endung .pdf\n",
    "glob_path = \"data/*.pdf\"\n",
    "\n",
    "# 📝 Lege einen leeren String an, um den extrahierten Text aller PDFs zu sammeln\n",
    "text = \"\"\n",
    "\n",
    "# 🔁 Durchlaufe alle PDF-Dateien im angegebenen Pfad mit Fortschrittsanzeige\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "\n",
    "    # 📄 Öffne die aktuelle PDF-Datei im Lese-/Binärmodus\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)  # Erstelle ein PDF-Reader-Objekt\n",
    "\n",
    "        # 🧾 Extrahiere den Text von allen Seiten, falls Text vorhanden ist\n",
    "        text += \" \".join(\n",
    "            page.extract_text()           # Text extrahieren\n",
    "            for page in reader.pages      # Für jede Seite im PDF\n",
    "            if page.extract_text()        # Nur wenn Text vorhanden ist (keine leere Seite)\n",
    "        )\n",
    "\n",
    "# 👁️ Zeige die ersten 50 Zeichen des zusammengefügten Textes als Vorschau\n",
    "text[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Importiere den Textsplitter (falls noch nicht geschehen)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 🔧 Erstelle einen Text-Splitter mit folgenden Parametern:\n",
    "# - chunk_size: max. 2000 Zeichen pro Chunk\n",
    "# - chunk_overlap: jeweils 200 Zeichen Überlappung zwischen zwei Chunks\n",
    "#   → sorgt dafür, dass der Kontext beim Übergang zwischen Chunks nicht verloren geht\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,         # Maximale Länge eines Chunks in Zeichen\n",
    "    chunk_overlap=200        # Überlappung zwischen benachbarten Chunks\n",
    ")\n",
    "\n",
    "# ✂️ Teile den extrahierten PDF-Text mithilfe des Splitters in kleinere, überlappende Textabschnitte\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# 🧾 Jetzt enthält die Variable 'chunks' eine Liste von Textabschnitten, die jeweils max. 2000 Zeichen lang sind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 130\n",
      "Preview of the first chunk: Asthma: diagnosis, \n",
      "moni toring and chr onic \n",
      "asthma manag emen t (BTS, \n",
      "NICE, SI GN) \n",
      "NICE guideline \n",
      "Published: 27 No vember 202 4 \n",
      "www .nice.or g.uk/guidance/ng2 45 \n",
      "© NICE 202 4. All right s reser\n"
     ]
    }
   ],
   "source": [
    "# 📊 Zeige die Gesamtanzahl der erzeugten Text-Chunks an\n",
    "# Das ist hilfreich zur Kontrolle, wie viele Abschnitte aus dem PDF-Text entstanden sind\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# 👁️‍🗨️ Zeige eine Vorschau auf den ersten Chunk (die ersten 200 Zeichen)\n",
    "# So kannst du kontrollieren, ob die Aufteilung sinnvoll funktioniert hat\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an embedding model\n",
    "Use the SentenceTransfomer wrapper as we have done so far.\n",
    "Models are found here: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "or on HuggingFace.\n",
    "\n",
    "Embed the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere den Namen des Embedding-Modells, das verwendet werden soll.\n",
    "# Dieses Modell wurde trainiert, um semantisch ähnliche Sätze in ähnliche Vektoren zu übersetzen.\n",
    "# \"multilingual\" bedeutet, dass es mit mehreren Sprachen (z. B. Englisch, Deutsch) umgehen kann.\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# Lade das ausgewählte Modell mit der SentenceTransformer-Bibliothek.\n",
    "# Es wird intern von HuggingFace geladen und kann sofort zur Vektorisierung verwendet werden.\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Erzeuge Embeddings (Vektoren) für alle Text-Chunks.\n",
    "# convert_to_numpy=True sorgt dafür, dass du ein NumPy-Array zurückbekommst (praktisch für spätere Verarbeitung).\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Index and save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "# 📐 Ermittle die Anzahl der Dimensionen eines einzelnen Embedding-Vektors\n",
    "# chunk_embeddings ist ein 2D-Array mit der Form (Anzahl der Chunks, Anzahl der Dimensionen)\n",
    "# z. B. (120, 384) → 120 Chunks, jeder als Vektor mit 384 Werten\n",
    "\n",
    "d = chunk_embeddings.shape[1]  # Index [1] gibt die Spaltenanzahl = Vektor-Dimension\n",
    "\n",
    "# 🖨️ Gib die Dimension des Embeddings aus (wichtig für FAISS oder Ähnlichkeitsvergleiche)\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔢 Was bedeutet das Ergebnis 384?\n",
    "Das Ergebnis 384 bedeutet, dass jedes deiner Chunks durch das Embedding-Modell in einen Vektor mit 384 Dimensionen umgewandelt wurde.\n",
    "\n",
    "🔍 Was heißt das konkret?\n",
    "Du hattest eine Liste mit vielen Chunks, z. B.:\n",
    "\n",
    "python\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "chunks = [\"Textabschnitt 1\", \"Textabschnitt 2\", ...]\n",
    "Mit dem Embedding-Modell:\n",
    "\n",
    "python\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "wurde jeder Chunk in einen Vektor umgerechnet.\n",
    "\n",
    "Jeder dieser Vektoren enthält 384 Zahlenwerte, z. B.:\n",
    "\n",
    "csharp\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "[0.12, -0.34, 0.87, ..., 0.01]  →  Länge: 384\n",
    "💡 Warum genau 384?\n",
    "Das liegt am verwendeten Modell:\n",
    "\n",
    "paraphrase-multilingual-MiniLM-L12-v2 erzeugt standardmäßig 384-dimensionale Embeddings.\n",
    "\n",
    "🔁 Andere Modelle liefern z. B.:\n",
    "\n",
    "768 (z. B. BERT-Base)\n",
    "\n",
    "1024 (z. B. RoBERTa-Large)\n",
    "\n",
    "📌 Bedeutung in der Praxis\n",
    "Du kannst dir jeden Vektor als Punkt in einem 384-dimensionalen Raum vorstellen.\n",
    "\n",
    "Zwei Chunks, die semantisch ähnlich sind, liegen nahe beieinander in diesem Raum.\n",
    "\n",
    "Das bildet die Grundlage für:\n",
    "\n",
    "✅ semantische Suche\n",
    "\n",
    "✅ Clustering\n",
    "\n",
    "✅ Klassifikation\n",
    "\n",
    "✅ Ähnlichkeitsvergleiche\n",
    "\n",
    "🧠 Wichtig für FAISS\n",
    "Wenn du z. B. mit FAISS einen Vektorindex aufbaust, musst du die Vektordimension angeben. In deinem Fall ist das:\n",
    "\n",
    "python\n",
    "Kopieren\n",
    "Bearbeiten\n",
    "d = 384\n",
    "FAISS erwartet diese Angabe, um korrekt mit den Vektoren arbeiten zu können.\n",
    "\n",
    "Möchtest du auch ein kurzes Beispiel mit np.linalg.norm() oder Cosinus-Ähnlichkeit zur Visualisierung von Vektorabstand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 130\n"
     ]
    }
   ],
   "source": [
    "# 📦 Erstelle einen FAISS-Index zur schnellen Ähnlichkeitssuche\n",
    "# Wir verwenden hier \"IndexFlatL2\", der auf der euklidischen Distanz (L2) basiert.\n",
    "# Der Parameter `d` gibt die Anzahl der Dimensionen pro Vektor an (z. B. 384 bei deinem Modell).\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# ➕ Füge alle zuvor erzeugten Embedding-Vektoren in den Index ein\n",
    "# Dadurch kann FAISS später Anfragen (Queries) mit diesen vergleichen\n",
    "index.add(chunk_embeddings)\n",
    "\n",
    "# 🔢 Gib aus, wie viele Vektoren im Index gespeichert sind\n",
    "# Sollte gleich der Anzahl deiner Chunks sein\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💾 Speichere den FAISS-Index auf der Festplatte\n",
    "# → Damit musst du den Index beim nächsten Mal nicht neu berechnen\n",
    "#    (spart Zeit beim späteren Wiederverwenden)\n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "\n",
    "# 🗂️ Speichere zusätzlich die Text-Chunks als Mapping (Index → Originaltext)\n",
    "# → So kannst du später zu jedem Treffer die zugehörige Textpassage finden\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)  # Serialisiere und speichere die Liste der Chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Key for language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔄 Lade Umgebungsvariablen aus einer .env-Datei (falls vorhanden)\n",
    "# Die Funktion `load_dotenv()` sucht nach einer Datei namens `.env` im Projektverzeichnis\n",
    "# und lädt alle darin enthaltenen Variablen als Umgebungsvariablen (environment variables).\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Lies den Google-API-Schlüssel aus der Umgebungsvariable GOOGLE_API_KEY\n",
    "# Wenn die Variable nicht gesetzt ist, wird `None` zurückgegeben – es erfolgt KEIN Fehler.\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# 🔐 Lies den OpenAI-API-Schlüssel aus der Umgebungsvariable OPENAI_API_KEY\n",
    "# Auch hier: kein Fehler, falls der Schlüssel nicht vorhanden ist.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 🔐 Lies den Groq-API-Schlüssel aus der Umgebungsvariable GROQ_API_KEY\n",
    "# Der Rückgabewert ist `None`, falls keine solche Umgebungsvariable existiert.\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 💡 Verhalten:\n",
    "# - Wenn keine `.env`-Datei vorhanden ist, oder eine Variable fehlt,\n",
    "#   dann sind die entsprechenden Variablen (`google_api_key`, etc.) = `None`.\n",
    "# - Das löst an dieser Stelle **noch keinen Fehler aus**.\n",
    "# - Fehler treten erst auf, wenn du z. B. versuchst, einen LLM-Client mit einem `None`-Key zu initialisieren.\n",
    "#\n",
    "# ✅ Optionaler Schutz (empfohlen):\n",
    "# assert google_api_key is not None, \"GOOGLE_API_KEY not found!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a retriever function\n",
    "\n",
    "arguments: query, k, index, chunks, embedding model\n",
    "\n",
    "return: retrieved texts, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build an answer function\n",
    "Build an answer function that takes a query, k, an index and the chunks.\n",
    "\n",
    "return: answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query(query, 5, index, chunks)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Rewriter\n",
    "\n",
    "Take a query and an api key for the model and rewrite the query. \n",
    "\n",
    "Rewriting a query: A Language Model is prompted to rewrite a query to better suit a task.\n",
    "\n",
    "Other Transfomrations are implemented in a similar fashion, this is just an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the rewriter into your answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query_with_rewriting(query, 5, index, chunks, groq_api_key)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 .Evaluation\n",
    "\n",
    "Select random chunks from all your chunks, and generate a question to each of these chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import httpx  # Ensure you're catching the correct timeout exception\n",
    "from openai import OpenAI\n",
    "def generate_questions_for_random_chunks(chunks, num_chunks=20, max_retries=3):\n",
    "    \"\"\"\n",
    "    Randomly selects a specified number of text chunks from the provided list,\n",
    "    then generates a question for each selected chunk using the Groq LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - num_chunks (int): Number of chunks to select randomly (default is 20).\n",
    "\n",
    "    Returns:\n",
    "    - questions (list of tuples): Each tuple contains (chunk, generated_question).\n",
    "    \"\"\"\n",
    "    # Randomly select the desired number of chunks.\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    \n",
    "    # Initialize the Groq client once\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    questions = []\n",
    "    for chunk in tqdm.tqdm(selected_chunks):\n",
    "        # Build a prompt that asks the LLM to generate a question based on the chunk.\n",
    "        prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        generated_question = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Try calling the API with simple retry logic.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                     model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "                generated_question = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred for chunk. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)  # Wait a bit before retrying.\n",
    "        \n",
    "        # If all attempts fail, use an error message as the generated question.\n",
    "        if generated_question is None:\n",
    "            generated_question = \"Error: Failed to generate question after several retries.\"\n",
    "        \n",
    "        questions.append((chunk, generated_question))\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = generate_questions_for_random_chunks(chunks, num_chunks=5, max_retries=2)\n",
    "for idx, (chunk, question) in enumerate(questions, start=1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk[:100]}...\\nGenerated Question: {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Test the questions with your built retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    For each (chunk, generated_question) tuple in the provided list, use the prebuilt\n",
    "    retrieval function to generate an answer for the generated question. The function\n",
    "    returns a list of dictionaries containing the original chunk, the generated question,\n",
    "    and the answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - question_tuples (list of tuples): Each tuple is (chunk, generated_question)\n",
    "    - k (int): Number of retrieved documents to use for answering.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list of dict): Each dict contains 'chunk', 'question', and 'answer'.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        # Use your retrieval-based answer function. Here we assume the function signature is:\n",
    "        # answer_query(query, k, index, texts, groq_api_key)\n",
    "        answer = answer_query(question, k, index, texts) #query, k, index,texts\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = answer_generated_questions(questions, 5, index, chunks, groq_api_key)\n",
    "\n",
    "for item in results:\n",
    "    print(\"Chunk Preview:\", item['chunk'][:100])\n",
    "    print(\"Generated Question:\", item['question'])\n",
    "    print(\"Answer:\", item['answer'])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_answers_binary(results, groq_api_key, max_retries=3):\n",
    "    \"\"\"\n",
    "    Evaluates each answer in the results list using an LLM.\n",
    "    For each result (a dictionary containing 'chunk', 'question', and 'answer'),\n",
    "    it sends an evaluation prompt to the Groq LLM which outputs 1 if the answer is on point,\n",
    "    and 0 if it is missing the point.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list of dict): Each dict must contain keys 'chunk', 'question', and 'answer'.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - max_retries (int): Maximum number of retries if the API call times out.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A dataframe containing the original chunk, question, answer, and evaluation score.\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    for item in tqdm.tqdm(results, desc=\"Evaluating Answers\"):\n",
    "        # Build the evaluation prompt.\n",
    "        prompt = (\n",
    "            \"Evaluate the following answer to the given question. \"\n",
    "            \"If the answer is accurate and complete, reply with 1. \"\n",
    "            \"If the answer is inaccurate, incomplete, or otherwise not acceptable, reply with 0. \"\n",
    "            \"Do not include any extra text.\\n\\n\"\n",
    "            \"Question: \" + item['question'] + \"\\n\\n\"\n",
    "            \"Answer: \" + item['answer'] + \"\\n\\n\"\n",
    "            \"Context (original chunk): \" + item['chunk'] + \"\\n\\n\"\n",
    "            \"Evaluation (1 for good, 0 for bad):\"\n",
    "        )\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        generated_eval = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Retry logic in case of timeouts or errors.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=\"4o-mini\"\n",
    "                )\n",
    "                generated_eval = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the retry loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred during evaluation. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error during evaluation: {e}. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # If no valid evaluation was produced, default to 0.\n",
    "        if generated_eval is None:\n",
    "            generated_eval = \"0\"\n",
    "        \n",
    "        # Convert the response to an integer (1 or 0).\n",
    "        try:\n",
    "            score = int(generated_eval)\n",
    "            if score not in [0, 1]:\n",
    "                score = 0\n",
    "        except:\n",
    "            score = 0\n",
    "        \n",
    "        evaluations.append(score)\n",
    "    \n",
    "    # Add the evaluation score to each result.\n",
    "    for i, item in enumerate(results):\n",
    "        item['evaluation'] = evaluations[i]\n",
    "    \n",
    "    # Create a dataframe for manual review.\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluations = evaluate_answers_binary(results, openai_api_key)\n",
    "display(df_evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
